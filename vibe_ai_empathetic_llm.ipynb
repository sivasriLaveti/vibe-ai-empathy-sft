{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_afbpHfpGi7u"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies for execution in Google Colab.\n",
        "# This notebook was developed and tested on a CUDA-enabled runtime.\n",
        "\n",
        "!pip install -q \\\n",
        "  torch torchvision torchaudio \\\n",
        "  transformers \\\n",
        "  datasets==2.18.0 \\\n",
        "  fsspec==2024.2.0 \\\n",
        "  peft \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EbIbmm0OA84"
      },
      "outputs": [],
      "source": [
        "# Imports & Environment\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMB_jh3eTk1-"
      },
      "outputs": [],
      "source": [
        "# Directory Setup\n",
        "\n",
        "import os\n",
        "\n",
        "# Create directories used throughout training.\n",
        "# - data/processed_safe : placeholder for any preprocessed or cached datasets\n",
        "# - checkpoints         : stores PEFT (LoRA) adapters and model checkpoints\n",
        "# - results             : stores training metrics and lightweight artifacts\n",
        "#\n",
        "# Using exist_ok=True makes the script idempotent and safe to re-run.\n",
        "dirs = [\n",
        "    \"data/processed_safe\",\n",
        "    \"checkpoints\",\n",
        "    \"results\"\n",
        "]\n",
        "\n",
        "for d in dirs:\n",
        "    os.makedirs(d, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyhLYx1zTpV2"
      },
      "outputs": [],
      "source": [
        "# Reproducibility Setup\n",
        "\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "import random\n",
        "\n",
        "# Fixed random seed for reproducibility of dataset shuffling and sampling\n",
        "SEED = 42\n",
        "random.seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sj5usdwTsQD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the EmpatheticDialogues training split.\n",
        "# This dataset contains multi-turn conversations focused on empathetic responses.\n",
        "raw_ed = load_dataset(\"empathetic_dialogues\", split=\"train\")\n",
        "\n",
        "def map_ed(x):\n",
        "    # Keep only user turns (speaker_idx == 0).\n",
        "    # Assistant turns are ignored in this simplified formulation.\n",
        "    if x[\"speaker_idx\"] != 0:\n",
        "        return {\"text\": None, \"emotion\": None, \"strategy\": None}\n",
        "\n",
        "    # Format the user utterance as a prompt for the assistant.\n",
        "    # Emotion and strategy labels are set to -1 since this dataset\n",
        "    # does not provide explicit supervision for these objectives.\n",
        "    return {\n",
        "        \"text\": f\"User: {x['utterance']}\\nAssistant:\",\n",
        "        \"emotion\": -1,\n",
        "        \"strategy\": -1\n",
        "    }\n",
        "\n",
        "# Apply the mapping function to all examples.\n",
        "ed = raw_ed.map(map_ed)\n",
        "\n",
        "# Filter out examples that were dropped during mapping.\n",
        "ed = ed.filter(lambda x: x[\"text\"] is not None)\n",
        "\n",
        "# Retain only the columns required for downstream training.\n",
        "ed = ed.remove_columns(\n",
        "    [c for c in ed.column_names if c not in [\"text\", \"emotion\", \"strategy\"]]\n",
        ")\n",
        "\n",
        "# Sanity check: verify schema and inspect one sample.\n",
        "print(ed.column_names)\n",
        "print(ed[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQpvcRu3zkr4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Mapping from ESConv strategy names to integer class IDs.\n",
        "# These IDs are used by the auxiliary strategy classification head.\n",
        "STRATEGY_MAP = {\n",
        "    \"Question\": 0,\n",
        "    \"Reflection of feelings\": 1,\n",
        "    \"Restatement or Paraphrasing\": 2,\n",
        "    \"Providing Suggestions\": 3,\n",
        "    \"Affirmation and Reassurance\": 4,\n",
        "    \"Information\": 5,\n",
        "    \"Others\": 6\n",
        "}\n",
        "\n",
        "def batched_map_es(batch):\n",
        "    # Containers for the flattened examples extracted from each conversation\n",
        "    texts = []\n",
        "    emotions = []\n",
        "    strategies = []\n",
        "\n",
        "    # Each entry in batch[\"text\"] is a JSON-encoded conversation\n",
        "    for raw_json in batch[\"text\"]:\n",
        "        data = json.loads(raw_json)\n",
        "\n",
        "        # Iterate over dialog turns within a conversation\n",
        "        for turn in data.get(\"dialog\", []):\n",
        "            # Keep only supporter (\"sys\") turns that have an associated strategy label\n",
        "            if turn.get(\"speaker\") == \"sys\" and \"strategy\" in turn:\n",
        "                strat = turn[\"strategy\"]\n",
        "\n",
        "                # Skip any strategy labels not covered by STRATEGY_MAP\n",
        "                if strat not in STRATEGY_MAP:\n",
        "                    continue\n",
        "\n",
        "                # Use a placeholder user prompt since the original user turn\n",
        "                # is not explicitly reconstructed in this mapping.\n",
        "                texts.append(\"User: <support needed>\\nAssistant:\")\n",
        "\n",
        "                # ESConv does not provide explicit emotion labels\n",
        "                emotions.append(-1)\n",
        "\n",
        "                # Convert strategy string into its integer class ID\n",
        "                strategies.append(STRATEGY_MAP[strat])\n",
        "\n",
        "    # Return a flattened batch suitable for HuggingFace dataset mapping\n",
        "    return {\n",
        "        \"text\": texts,\n",
        "        \"emotion\": emotions,\n",
        "        \"strategy\": strategies\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iAxbTgaTvxm"
      },
      "outputs": [],
      "source": [
        "# Load the ESConv training split.\n",
        "# This dataset contains emotional support conversations with strategy annotations.\n",
        "raw_es = load_dataset(\"thu-coai/esconv\", split=\"train\")\n",
        "\n",
        "# Apply the batched mapping function to extract supporter turns\n",
        "# and convert strategy labels into integer IDs.\n",
        "# All original columns are removed to retain only the processed fields.\n",
        "es = raw_es.map(\n",
        "    batched_map_es,\n",
        "    batched=True,\n",
        "    remove_columns=raw_es.column_names\n",
        ")\n",
        "\n",
        "# Sanity check: verify schema and inspect one processed example.\n",
        "print(es.column_names)\n",
        "print(es[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dpkykS67hsG"
      },
      "outputs": [],
      "source": [
        "# Load the GoEmotions training split.\n",
        "# This dataset provides fine-grained emotion labels for single user utterances.\n",
        "raw_ge = load_dataset(\"go_emotions\", split=\"train\")\n",
        "\n",
        "def map_ge(x):\n",
        "    # Select the first emotion label if available.\n",
        "    # Samples without labels are assigned -1 to indicate missing supervision.\n",
        "    label = x[\"labels\"][0] if len(x[\"labels\"]) > 0 else -1\n",
        "\n",
        "    # Format the user utterance as an assistant prompt.\n",
        "    # Strategy labels are set to -1 since GoEmotions does not provide them.\n",
        "    return {\n",
        "        \"text\": f\"User: {x['text']}\\nAssistant:\",\n",
        "        \"emotion\": int(label),\n",
        "        \"strategy\": -1\n",
        "    }\n",
        "\n",
        "# Apply the mapping function and drop all original columns.\n",
        "ge = raw_ge.map(map_ge, remove_columns=raw_ge.column_names)\n",
        "\n",
        "# Sanity check: verify the resulting schema.\n",
        "print(ge.column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSGGNvP9xU8Y"
      },
      "outputs": [],
      "source": [
        "# Combine the processed EmpatheticDialogues, GoEmotions, and ESConv datasets\n",
        "# into a single training dataset. Each source contributes a different\n",
        "# supervision signal (generation prompts, emotion labels, or strategy labels).\n",
        "dataset = concatenate_datasets([ed, ge, es])\n",
        "\n",
        "# Sanity checks: inspect dataset size, schema, and one example.\n",
        "print(dataset)\n",
        "print(dataset.column_names)\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # or 3B if you switched\n",
        "MAX_LEN = 128"
      ],
      "metadata": {
        "id": "fumbIh9zSmAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLQTyTrU7Se_"
      },
      "outputs": [],
      "source": [
        "# Base model used for tokenization and fine-tuning.\n",
        "# The same tokenizer is reused during training and inference.\n",
        "\n",
        "\n",
        "# Load the tokenizer associated with the base model.\n",
        "# The pad token is explicitly set to EOS to avoid padding-related issues\n",
        "# with causal language models.\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def tokenize(example):\n",
        "    # Tokenize the input text prompt.\n",
        "    # Padding and truncation are applied to ensure fixed-length inputs\n",
        "    # compatible with batched training.\n",
        "    t = tokenizer(\n",
        "        text=example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "    # Return tokenized inputs along with auxiliary labels.\n",
        "    # Emotion and strategy labels may be -1 to indicate missing supervision.\n",
        "    return {\n",
        "        \"input_ids\": t[\"input_ids\"],\n",
        "        \"attention_mask\": t[\"attention_mask\"],\n",
        "        \"emotion\": int(example[\"emotion\"]),\n",
        "        \"strategy\": int(example[\"strategy\"])\n",
        "    }\n",
        "\n",
        "# Apply tokenization to the full mixed dataset and remove raw text columns.\n",
        "tokenized = dataset.map(\n",
        "    tokenize,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNa60fHz_WM6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the base causal language model with 4-bit quantization\n",
        "# to reduce memory usage during fine-tuning.\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Configure LoRA for parameter-efficient fine-tuning.\n",
        "# Only attention projection layers are adapted.\n",
        "lora = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Wrap the base model with LoRA adapters.\n",
        "model = get_peft_model(base_model, lora)\n",
        "\n",
        "\n",
        "class AuxHeads(nn.Module):\n",
        "    def __init__(self, hidden, emo=28, strat=7):\n",
        "        super().__init__()\n",
        "        # Linear head for emotion classification\n",
        "        self.emo = nn.Linear(hidden, emo)\n",
        "        # Linear head for support strategy classification\n",
        "        self.strat = nn.Linear(hidden, strat)\n",
        "\n",
        "    def forward(self, h):\n",
        "        # Use the hidden state of the final token as a pooled representation\n",
        "        # for auxiliary classification tasks.\n",
        "        pooled = h[:, -1, :]\n",
        "        return self.emo(pooled), self.strat(pooled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KNP6ooyBmnf"
      },
      "outputs": [],
      "source": [
        "# Instantiate auxiliary heads and move them to the same device as the model.\n",
        "\n",
        "aux = AuxHeads(model.config.hidden_size).to(\n",
        "    device=model.device,\n",
        "    dtype=model.dtype\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4q7_8O7_6co"
      },
      "outputs": [],
      "source": [
        "# Select a small batch of examples for a forward-pass sanity check.\n",
        "# This helps verify tensor shapes, device placement, and loss computation\n",
        "# before launching full training.\n",
        "batch = tokenized.select(range(2))\n",
        "\n",
        "# Move inputs and labels to the same device as the model.\n",
        "input_ids = torch.tensor(batch[\"input_ids\"]).to(model.device)\n",
        "mask = torch.tensor(batch[\"attention_mask\"]).to(model.device)\n",
        "emotion = torch.tensor(batch[\"emotion\"]).to(model.device)\n",
        "strategy = torch.tensor(batch[\"strategy\"]).to(model.device)\n",
        "\n",
        "# Run a forward pass through the base model and request hidden states\n",
        "# for use by the auxiliary classification heads.\n",
        "outputs = model(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=mask,\n",
        "    output_hidden_states=True\n",
        ")\n",
        "\n",
        "# Extract the final hidden layer representations.\n",
        "hidden = outputs.hidden_states[-1]\n",
        "\n",
        "# Compute emotion and strategy logits using the auxiliary heads.\n",
        "emo_logits, strat_logits = aux(hidden)\n",
        "\n",
        "def masked_ce(logits, targets, ignore_index=-1):\n",
        "    # Compute cross-entropy loss while ignoring samples\n",
        "    # with missing supervision (label == ignore_index).\n",
        "    valid = targets != ignore_index\n",
        "    if valid.sum() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "    return F.cross_entropy(logits[valid], targets[valid])\n",
        "\n",
        "# Compute auxiliary losses for emotion and strategy prediction.\n",
        "emo_loss = masked_ce(emo_logits, emotion)\n",
        "strat_loss = masked_ce(strat_logits, strategy)\n",
        "\n",
        "# Print losses as a sanity check to confirm valid forward computation.\n",
        "print(\"Emotion loss:\", emo_loss.item())\n",
        "print(\"Strategy loss:\", strat_loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the base model and auxiliary heads to training mode.\n",
        "# This enables training-specific behavior such as dropout.\n",
        "model.train()\n",
        "aux.train()\n"
      ],
      "metadata": {
        "id": "Tm6awXvC9gD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_ce(logits, targets, ignore_index=-1):\n",
        "    # Compute cross-entropy loss while ignoring targets marked with ignore_index.\n",
        "    # This allows mixing examples with and without auxiliary supervision\n",
        "    # in the same batch.\n",
        "    valid = targets != ignore_index\n",
        "    if valid.sum() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "    return F.cross_entropy(logits[valid], targets[valid])\n",
        "\n",
        "class MultiTaskTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        # Forward pass through the base language model.\n",
        "        # Hidden states are requested for use by auxiliary heads.\n",
        "        outputs = model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        # Extract the final hidden layer representations.\n",
        "        hidden = outputs.hidden_states[-1]\n",
        "\n",
        "        # Compute auxiliary task logits using the shared hidden states.\n",
        "        emo_logits, strat_logits = aux(hidden)\n",
        "\n",
        "        # Compute masked auxiliary losses.\n",
        "        # Samples without emotion or strategy labels are ignored.\n",
        "        emo_loss = masked_ce(emo_logits, inputs[\"emotion\"])\n",
        "        strat_loss = masked_ce(strat_logits, inputs[\"strategy\"])\n",
        "\n",
        "        # Combine auxiliary losses into a single scalar loss.\n",
        "        # Note: language modeling loss is intentionally excluded here.\n",
        "        loss = emo_loss + strat_loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "1YnjAN0_9m4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "# Use HuggingFace's default data collator to batch inputs.\n",
        "# This handles padding and tensor conversion for standard fields\n",
        "# such as input_ids, attention_mask, and auxiliary labels.\n",
        "def data_collator(features):\n",
        "    return default_data_collator(features)\n"
      ],
      "metadata": {
        "id": "0NIHNHDvKt6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training hyperparameters and runtime configuration.\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./checkpoints\",            # Directory for saving checkpoints and logs\n",
        "    per_device_train_batch_size=1,         # Small batch size to fit large models in memory\n",
        "    gradient_accumulation_steps=16,        # Accumulate gradients to simulate a larger batch\n",
        "    num_train_epochs=1,                    # Single epoch for a time-bounded assignment run\n",
        "    fp16=False,                            # Disabled to avoid mixed-precision instability\n",
        "    bf16=False,                            # Disabled for broader hardware compatibility\n",
        "    logging_steps=50,                      # Log training metrics every N steps\n",
        "    save_strategy=\"no\",                    # Disable checkpoint saving during training\n",
        "    report_to=\"none\",                      # Disable external logging integrations\n",
        "    remove_unused_columns=False            # Ensure auxiliary labels are preserved\n",
        ")\n"
      ],
      "metadata": {
        "id": "RiXpkT3z92VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the custom Trainer with the multi-task loss definition.\n",
        "# The Trainer orchestrates batching, optimization, and logging.\n",
        "trainer = MultiTaskTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ],
      "metadata": {
        "id": "2hcn1oek9rH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print data types to verify consistency across model components.\n",
        "# This is especially important when using quantization or reduced precision\n",
        "# to avoid silent dtype mismatches during training.\n",
        "print(\"Model dtype:\", model.dtype)\n",
        "print(\"Hidden dtype:\", hidden.dtype)\n",
        "print(\"Aux dtype:\", next(aux.parameters()).dtype)\n"
      ],
      "metadata": {
        "id": "rPHGLf509wZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable gradient checkpointing to reduce memory usage during training.\n",
        "# This is important when fine-tuning large models with limited GPU memory.\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(\"Ready to train\")\n"
      ],
      "metadata": {
        "id": "hhsBRTWPFkZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()\n"
      ],
      "metadata": {
        "id": "XsF0S0p2LO3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained LoRA adapter weights.\n",
        "# These adapters contain all learned parameters from fine-tuning,\n",
        "# while the base model remains unchanged and is reloaded at inference time.\n",
        "model.save_pretrained(\"results/lora_adapter\")\n",
        "\n",
        "# Save the tokenizer to ensure consistency between training and inference.\n",
        "# This avoids token mismatch issues when loading the model later.\n",
        "tokenizer.save_pretrained(\"results/lora_adapter\")\n",
        "\n",
        "\n",
        "# Persist lightweight training metadata for reference.\n",
        "# This records loss values and logging information produced during training\n",
        "# and serves as evidence that the training run completed successfully.\n",
        "with open(\"results/train_metrics.json\", \"w\") as f:\n",
        "    json.dump(trainer.state.log_history, f, indent=2)\n",
        "\n",
        "\n",
        "print(\"Training complete. LoRA adapters and metrics saved to the results/ directory.\")\n"
      ],
      "metadata": {
        "id": "aODSej4aOcJj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
